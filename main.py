import streamlit as st
import random
import re
import tempfile
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from DB.insert import insert_data
import openai

####################### ë©”ì¸ í™”ë©´ ì„¸íŒ… #######################

st.set_page_config(page_title="í•œê±¸ìŒAI í”„ë¡œí† íƒ€ì…")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "sidebar_history" not in st.session_state:
    st.session_state.sidebar_history = []
if "full_history" not in st.session_state:
    st.session_state.full_history = []
if "session_active" not in st.session_state:
    st.session_state.session_active = False

st.title("ë°”ë¼ë´‡")
st.title('ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?')

####################### ì‚¬ì´ë“œë°” #######################

st.sidebar.title("ì§ˆë¬¸ ì´ë ¥ :book:")
st.sidebar.write("---")

# "ìƒˆë¡œìš´ ì§ˆë¬¸í•˜ê¸°" ë²„íŠ¼
if st.sidebar.button("ìƒˆë¡œìš´ ì§ˆë¬¸ í•˜ê¸°â•"):

    if st.session_state.chat_history and not st.session_state.get('restored_session', False):
        first_user_question = next((msg for msg in st.session_state.chat_history if msg["role"] == "user"), None)
        if first_user_question:
            st.session_state.sidebar_history.append(first_user_question)
        st.session_state.full_history.append(st.session_state.chat_history.copy())
    
    st.session_state.chat_history = []
    st.session_state.restored_session = False

for idx, question in enumerate(st.session_state.sidebar_history):
    if st.sidebar.button(f"{idx + 1}. {question['content']}"):
        st.session_state.chat_history = st.session_state.full_history[idx]
        st.session_state.restored_session = True

####################### íŒŒì¼ ì—…ë¡œë“œ ë° GPT ì„¤ì • #######################

uploaded_file = st.file_uploader("í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”!", type=['txt'])

def txt_to_document(uploaded_file):
    temp_dir = tempfile.TemporaryDirectory()
    temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)
    with open(temp_filepath, "wb") as f:
        f.write(uploaded_file.getvalue())
    loader = TextLoader(temp_filepath)
    pages = loader.load_and_split()
    return pages

if uploaded_file is not None:
    pages = txt_to_document(uploaded_file)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)
    texts = text_splitter.split_documents(pages)

    embeddings_model = OpenAIEmbeddings()
    db = Chroma.from_documents(texts, embeddings_model)

    st.header("ì–´ë–¤ ì§ˆë¬¸ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

    # ì—­í•  í”„ë¡¬í”„íŠ¸ ì„¤ì • 
    role_prompt = "ê²½ê³„ì„± ì§€ëŠ¥ ì¥ì• ê°€ ìˆëŠ” ì‚¬ëŒì„ ìœ„í•´ì„œ ìœ ì¹˜ì› ìˆ˜ì¤€ì—ë° ì„¤ëª…í•˜ë“¯ ë§¤ìš° ì‰¬ìš´ ë‚œì´ë„ë¡œ ì†Œí†µí•´ ì£¼ë˜, ë‹µë³€ì€ ìµœëŒ€í•œ ê°„ëµí•˜ê²Œ ë¶€íƒí•´ìš”. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¹œêµ¬ ì—­í• ë¡œ ëŒ€í™”í•´ ì£¼ì„¸ìš”."

    # ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥ (ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹ˆ ë¬¸ìì—´ì„ ì‚¬ìš©)
    question = st.text_input('ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”', value='') 

    # ì‚¬ìš©ìê°€ ë²”ì£„ ê´€ë ¨ ì§ˆë¬¸ì„ í–ˆëŠ”ì§€ ê°ì§€ (ì˜ˆë¥¼ ë“¤ì–´, 'ì‚¬ê¸°', 'ìœ„í˜‘' ë“±ì˜ ë‹¨ì–´ë¥¼ í¬í•¨)
    crime_keywords = ['ì‚¬ê¸°', 'ìœ„í˜‘', 'ë„ë‘‘', 'ë²”ì£„', 'í•´í‚¹', 'ë³´ì´ìŠ¤í”¼ì‹±', 'ì‚¬ì¹­']

    # 'í€´ì¦ˆ'ë¼ê³  ì…ë ¥í–ˆì„ ë•Œ í€´ì¦ˆ ê¸°ëŠ¥ ì‹¤í–‰
    if question.lower() == 'í€´ì¦ˆ':
        st.write("í€´ì¦ˆë¥¼ ì‹œì‘í•©ë‹ˆë‹¤! ëœë¤ í€´ì¦ˆë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")

        # GPT í€´ì¦ˆ ìƒì„± ë¡œì§
        def generate_quiz():
            prompt = f"""
            {role_prompt}
            ë‹¹ì‹ ì€ ê²½ê³„ì„± ì§€ëŠ¥ ì¥ì• ê°€ ìˆëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•œ í€´ì¦ˆë¥¼ ì¶œì œí•˜ëŠ” AIì…ë‹ˆë‹¤. ì´ í€´ì¦ˆëŠ” ìœ„í—˜í•œ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ë¬»ëŠ” í€´ì¦ˆì…ë‹ˆë‹¤. ìƒí™©ì„ ì£¼ê³ , 3ê°œì˜ ì„ íƒì§€ë¥¼ ì œê³µí•˜ê³ , ì •ë‹µê³¼ í•´ì„¤ë„ ì œê³µí•©ë‹ˆë‹¤.
            
            ì˜ˆì‹œ:
            ìƒí™©: "ê°€ê¹Œìš´ ì¹œêµ¬ê°€ â€˜ê¸‰í•˜ê²Œ ëˆì´ í•„ìš”í•˜ë‹¤â€™ë©° ë©”ì‹ ì €ë¡œ ëˆì„ ë³´ë‚´ë‹¬ë¼ê³  ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì´ëŸ´ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?"
            1. ë°”ë¡œ ëˆì„ ì†¡ê¸ˆí•œë‹¤.
            2. ì¹œêµ¬ì—ê²Œ ì§ì ‘ ì „í™”í•´ ì‚¬ì‹¤ì„ í™•ì¸í•œë‹¤.
            3. ë©”ì‹ ì €ë¡œ ì¶”ê°€ ì§ˆë¬¸ì„ í•´ ìƒí™©ì„ íŒŒì•…í•œë‹¤.
            ì •ë‹µ: 2ë²ˆ
            í•´ì„¤: ì§€ì¸ì„ ì‚¬ì¹­í•œ ì‚¬ê¸°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë©”ì‹ ì €ë¡œ ìš”ì²­ë°›ì€ ëˆì€ ë°”ë¡œ ë³´ë‚´ì§€ ë§ê³ , ë°˜ë“œì‹œ ì§ì ‘ í™•ì¸ í›„ í–‰ë™í•´ì•¼ í•©ë‹ˆë‹¤.

            ìƒˆë¡œìš´ í€´ì¦ˆë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.
            """
            response = openai.Completion.create(
                engine="text-davinci-003",
                prompt=prompt,
                max_tokens=300,
                temperature=0.5,
            )
            return response.choices[0].text

        # í€´ì¦ˆ ìƒì„± ë° ì¶œë ¥
        quiz = generate_quiz()
        st.write(quiz)

        # ì‚¬ìš©ìê°€ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ ì…ë ¥
        user_answer = st.text_input("ì •ë‹µì„ ì…ë ¥í•˜ì„¸ìš” (ìˆ«ìë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”)")

        # ì œì¶œ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì •ë‹µì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ
        if st.button("ì œì¶œ"):
            # GPTì—ê²Œ ì •ë‹µì„ í‰ê°€í•˜ë„ë¡ ìš”ì²­
            def evaluate_answer(user_answer, quiz_question):
                prompt = f"""
                {role_prompt}
                ë‹¤ìŒ í€´ì¦ˆì— ëŒ€í•œ ì‚¬ìš©ìì˜ ë‹µë³€ì„ í‰ê°€í•´ ì£¼ì„¸ìš”.

                í€´ì¦ˆ:
                {quiz_question}

                ì‚¬ìš©ìì˜ ë‹µë³€: {user_answer}

                ì •ë‹µ ì—¬ë¶€ì™€ í•´ì„¤ì„ ì œê³µí•´ ì£¼ì„¸ìš”.
                """
                response = openai.Completion.create(
                    engine="text-davinci-003",
                    prompt=prompt,
                    max_tokens=150,
                    temperature=0.5,
                )
                return response.choices[0].text

            result = evaluate_answer(user_answer, quiz)
            st.write(result)

    # ë²”ì£„ ê´€ë ¨ ì§ˆë¬¸ì¼ ê²½ìš° ë¯¼ê°í•œ ëŒ€ì‘
    elif any(keyword in question for keyword in crime_keywords):
        st.write("ì´ëŸ° ìƒí™©ì€ ë§¤ìš° ì¤‘ìš”í•œ ë¬¸ì œì…ë‹ˆë‹¤. ì œê°€ ë„ì›€ì„ ë“œë¦´ê²Œìš”...")

        # GPTë¥¼ ì‚¬ìš©í•˜ì—¬ txt íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        with st.spinner('ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'):
            prompt = f"{role_prompt}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"
            llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
            retriever = db.as_retriever()
            qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)
            result = qa_chain({"query": question})

            new_message = {"role": "user", "content": question}
            st.session_state.chat_history.append(new_message)
            new_response = {"role": "chatbot", "content": result["result"]}
            st.session_state.chat_history.append(new_response)
            insert_data(question, result["result"])

            st.write(result["result"])

    elif question:
        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ì²˜ë¦¬
        with st.spinner('ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'):
            prompt = f"{role_prompt}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"
            llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
            retriever = db.as_retriever()
            qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)
            result = qa_chain({"query": question})

            new_message = {"role": "user", "content": question}
            st.session_state.chat_history.append(new_message)
            new_response = {"role": "chatbot", "content": result["result"]}
            st.session_state.chat_history.append(new_response)
            insert_data(question, result["result"])

            st.write(result["result"])

# ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ì´ì „ ë©”ì‹œì§€ë“¤ í‘œì‹œ
if not question:  # questionì´ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì´ì „ ê¸°ë¡ì„ í‘œì‹œ
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"], avatar="ğŸ»" if message["role"] == "chatbot" else None):
            st.write(message["content"])
